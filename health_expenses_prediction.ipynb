{"cells":[{"cell_type":"markdown","metadata":{},"source":["# PROJECT: PREDICTING MEDICAL EXPENSES. \n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## Required libraries\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from scipy import stats\n","from scipy.stats.mstats import normaltest\n","from scipy.stats import boxcox\n","import seaborn as sns\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import (StandardScaler, PolynomialFeatures)\n","from sklearn.linear_model import LinearRegression,Ridge,Lasso,ElasticNet\n","from sklearn.metrics import r2_score \n","from sklearn.model_selection import KFold, cross_val_predict, cross_val_score\n","import matplotlib.pyplot as plt\n","import ipywidgets as widget\n","from ipywidgets import interact\n"]},{"cell_type":"markdown","metadata":{},"source":["## Dataset\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data = pd.read_csv(r\"C:\\Users\\BAYEE\\Desktop\\Online Courses\\IBM Machine Learning Engineer\\1. Regression\\final project_regression\\insurance.csv\", encoding='ISO-8859-1')\n","data.head()"]},{"cell_type":"markdown","metadata":{},"source":["# About the Data\n"]},{"cell_type":"markdown","metadata":{},"source":["The insurance dataset comprises 1,338 observations and 7 distinct features represented as columns. This dataset encompasses 4 numerical attributes, namely age, BMI, children, and expenses, along with 3 categorical attributes denoted as sex, smoker, and region."]},{"cell_type":"markdown","metadata":{},"source":["# Objectives\n"]},{"cell_type":"markdown","metadata":{},"source":["The primary aim of this project is to predict the medical expenses of individuals, a task that aids medical insurance providers in determining their premium charges."]},{"cell_type":"markdown","metadata":{},"source":["# Data Cleaning\n"]},{"cell_type":"markdown","metadata":{},"source":["Missing Values"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# no missing data\n","data.info()\n","data.isnull().sum()\n","#no missing values"]},{"cell_type":"markdown","metadata":{},"source":["Outliers"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["numeric_columns = data.select_dtypes(include=[np.number]).columns.tolist() #creating number columns\n","qualitative_columns=data.select_dtypes(include=[np.object_]).columns.tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#used box plot to detect outliers\n","fig, axes = plt.subplots(nrows=1, ncols=len(numeric_columns), figsize=(15, 6))  # Create subplots\n","\n","for i, column in enumerate(numeric_columns):\n","    axes[i].boxplot(data[column], vert=False)  # Create a box plot for the column in the i-th subplot\n","    axes[i].set_title(f'Box Plot for {column}')  # Set the title for the subplot\n","    axes[i].set_xlabel(column)  # Set the x-axis label\n","\n","plt.tight_layout()  # Adjust subplot layout for better spacing\n","plt.show()  # Display the figure with subplots\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#used z-score to detect and remove outliers\n","\n","treshold=3\n","for column in numeric_columns:\n","    # Calculate the z-scores\n","    z_scores = stats.zscore(data[column])\n","    \n","    # Find the rows where z-scores are greater than treshold\n","    outliers = np.abs(z_scores) > treshold\n","    \n","    # Replace outliers with the mean value of the column\n","    mean_value = data[column].mean()\n","    data.loc[outliers, column] = mean_value\n","\n","# Now, df contains the data with outliers replaced by the mean value for each numeric column\n","data"]},{"cell_type":"markdown","metadata":{},"source":["Normality of the Dependent Variable"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#function to return plots for the feature\n","def normality(data,feature):\n","    plt.figure(figsize=(10,5))\n","    plt.subplot(1,2,1)\n","    sns.kdeplot(data[feature])\n","    plt.subplot(1,2,2)\n","    stats.probplot(data[feature],dist=\"norm\", plot=plt)\n","    plt.show()\n","    statistic, pvalue=normaltest(np.log(data[feature])) # not normally distributed\n","    print(\"Statistic = {}\".format(statistic))\n","    print(\"pvalue = {}\".format(pvalue))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## Log transforation of numeric olumns to make the data more normal\n","for col in numeric_columns:\n","    data[\"log_\"+col]=np.log1p(data[col])\n","#data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["normality(data, 'expenses')\n","normality(data, 'log_expenses')  #the log transformed data looks more normal"]},{"cell_type":"markdown","metadata":{},"source":["One Hot Encoding"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# Get a Pd.Series consisting of all the string categoricals\n","one_hot_encode_cols = data.dtypes[data.dtypes == object]  # filtering by string categoricals\n","one_hot_encode_cols = one_hot_encode_cols.index.tolist()  # list of categorical fields\n","\n","# Encode these columns as categoricals so one hot encoding works on split data (if desired)\n","for col in one_hot_encode_cols:\n","    data[col] = pd.Categorical(data[col])\n","\n","# Do the one hot encoding\n","data = pd.get_dummies(data, columns=one_hot_encode_cols, drop_first=True)\n","data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Drop the non-log numerical columns\n","data.drop(numeric_columns, axis=1, inplace=True)\n","data"]},{"cell_type":"markdown","metadata":{},"source":["# 3. Linear Regression Models\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## Defining Target and Feature Variables\n","y=data['log_expenses']\n","X=data.drop('log_expenses', axis=1)\n","\n","## Spliting data into training and testing\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n","\n","## Normalizing the training data using `StandardScaler` on `X_train`. Use fit_transform() function**\n","s = StandardScaler()\n","X_train = s.fit_transform(X_train)\n","\n","X_test=s.transform(X_test)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## Linear Regression\n","lm = LinearRegression() \n","lm.fit(X_train,y_train) \n","lm_predictions = lm.predict(X_test)\n","r2_lm=r2_score(y_test,lm_predictions)\n","\n","# ## Ridge Regression\n","rr=Ridge()\n","rr.fit(X_train, y_train)\n","rr_predictions = rr.predict(X_test)\n","r2_rr=r2_score(y_test, rr_predictions)\n","\n","# ## Lasso Regression\n","ls=Lasso()\n","ls.fit(X_train, y_train)\n","ls_predictions = rr.predict(X_test)\n","r2_ls=r2_score(y_test, ls_predictions)\n","\n","# ## Elastic Net Regression\n","en=ElasticNet()\n","en.fit(X_train, y_train)\n","en_predictions = en.predict(X_test)\n","r2_en=r2_score(y_test, en_predictions)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"R-squared results\")\n","print(\"*******************************************************\")\n","print(\"linear Regression = {}\".format(r2_lm))\n","print(\"ridge Regression = {}\".format(r2_rr))\n","print(\"lasso Regression = {}\".format(r2_ls))\n","print(\"elastic net Regression = {}\".format(r2_en))"]},{"cell_type":"markdown","metadata":{},"source":["Visual representation of actual and predicted values"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def  plot_dis(y,yhat):\n","    \n","    plt.figure()\n","    ax1 = sns.distplot(y, hist=False, color=\"r\", label=\"Actual Value\")\n","    sns.distplot(yhat, hist=False, color=\"b\", label=\"Fitted Values\" , ax=ax1)\n","    plt.legend()\n","\n","    plt.title('Actual vs Fitted Values')\n","    # plt.xlabel('Price (in dollars)')\n","    # plt.ylabel('Proportion of Cars')\n","\n","    plt.show()\n","    plt.close()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_dis(y,ls_predictions)"]},{"cell_type":"markdown","metadata":{},"source":["## Deep Learning - Artificial Neural Networks"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.metrics import MeanSquaredError\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from tensorflow_addons.metrics import RSquare\n","\n","# Assuming X_train, X_test, y_train, y_test are your training and testing data\n","# Assuming they are numpy arrays\n","\n","# Normalize your input data\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)\n","\n","# Define your neural network architecture\n","ann = Sequential([\n","  Dense(64, activation='relu', input_shape=(8,)),\n","  Dense(32, activation='relu'),\n","  Dense(1)\n","])\n","\n","# Compile the model with Mean Squared Error loss and RSquare metric\n","ann.compile(optimizer='adam', loss='mean_squared_error', metrics=[RSquare()])\n","\n","# Fit the model to your training data\n","# model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test))\n","history = ann.fit(X_train, y_train, epochs=75, validation_data=(X_test, y_test))\n","\n","# Evaluate the model on the testing data\n","loss, r2 = ann.evaluate(X_test, y_test)\n","\n","print(\"Mean Squared Error on Test Data:\", loss)\n","print(\"R-squared on Test Data:\", r2)\n","\n","#plot the training and validation accuracy and loss at each epoch\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","epochs = range(1,len(loss) + 1)\n","plt.plot(epochs, loss, 'y', label = 'Training loss')\n","plt.plot(epochs, val_loss, 'r', label = 'Validation loss')\n","plt.title('Training and Validation loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["# Insights and key findings\n"]},{"cell_type":"markdown","metadata":{},"source":["1. With the exception of Lassor Regression, the the R-squared of the linear regression models are generally 77%\n","2. The R-squared of the Artificial Neural Networks (ANN) model is about 80%\n","2. Therefore, ANN model was selected for this project"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":4}
